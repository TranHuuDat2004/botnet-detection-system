{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6376134,"sourceType":"datasetVersion","datasetId":3674161}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"toc_visible":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction  \n\nThe **CICIDS2017 dataset** is a comprehensive collection of network traffic data, designed for evaluating intrusion detection systems (IDS). This notebook focuses on preparing the dataset for machine learning (ML) models by implementing a thorough data cleaning and treatment pipeline.  \n\nThe initial analysis involved exploratory data analysis (EDA) to uncover the dataset's structure and key characteristics. Building on these insights, this notebook ensures the dataset is ready for effective model training and evaluation by addressing issues such as data inconsistencies, feature overlapping, and output preparation.  \n\n### Objectives of This Notebook:\n1. **Data Cleaning**:\n   - Handle missing values through removal or imputation.  \n   - Eliminate duplicates and correct inconsistencies to maintain data quality.  \n\n2. **Exploratory Data Analysis (EDA)**:  \n   - Visualize and analyze the dataset to identify patterns, outliers, and correlations.  \n\n3. **Feature Importance Evaluation**:\n   - Evaluate feature importance with statistical tests and Random Forest.\n\n4. **Dataset Preparation for Modeling**:  \n   - Save a **cleaned dataset** for baseline model training.  \n\nThis pipeline ensures the CICIDS2017 dataset is refined for robust machine learning experiments.","metadata":{"id":"wczGN1xeEvTb"}},{"cell_type":"code","source":"# Importing the relevant libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom scipy import stats","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-10-17T04:43:59.233922Z","iopub.execute_input":"2025-10-17T04:43:59.234347Z","iopub.status.idle":"2025-10-17T04:44:01.100747Z","shell.execute_reply.started":"2025-10-17T04:43:59.234284Z","shell.execute_reply":"2025-10-17T04:44:01.099640Z"},"id":"50-TJ8koEvTc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Helper Functions\n\nSection to organize some useful functions to further explore and work on the dataset.","metadata":{"id":"31gsv0VNJ4hr"}},{"cell_type":"code","source":"# 1. Get numerical and categorical features (df, target_col='Attack Type') -> numerical features; categorical features\ndef get_feature_types(df, target_col='Attack Type'):\n    \"\"\"\n    Identify numeric and categorical features, if present, in the dataset.\n\n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        Input DataFrame containing the dataset\n    target_col : str, optional (default='Attack Type')\n        Name of the target column to exclude from features\n\n    Returns:\n    --------\n    tuple : (list, list)\n        Two lists containing:\n        - numeric_features: List of column names with numerical data\n        - categorical_features: List of column names with categorical data\n\n    Notes:\n    ------\n    - Numerical features are identified as columns containing numeric data types\n    - Categorical features are identified as columns containing object data types\n    - The target column is excluded from both feature lists if present\n    \"\"\"\n\n    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n\n    # Remove target column if present\n    if target_col in numeric_features:\n        numeric_features.remove(target_col)\n    if target_col in categorical_features:\n        categorical_features.remove(target_col)\n\n    return numeric_features, categorical_features","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:01.102049Z","iopub.execute_input":"2025-10-17T04:44:01.102466Z","iopub.status.idle":"2025-10-17T04:44:01.109477Z","shell.execute_reply.started":"2025-10-17T04:44:01.102435Z","shell.execute_reply":"2025-10-17T04:44:01.107957Z"},"id":"b0k9A1SiKBDp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Correlation analysis (df, numeric_features) -> list of tuples with highly correlaed features\ndef correlation_analysis(df, numeric_features, threshold=0.85):\n    \"\"\"\n    Analyze correlations between numerical features.\n\n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        Input DataFrame containing the dataset\n    numeric_features : list\n        List of column names containing numerical features to analyze\n    threshold : float\n        Threshold for determining highly correlated features (default='0.85')\n\n    Returns:\n    --------\n    list of tuples\n        List of highly correlated feature pairs and their correlation values\n        Each tuple contains (feature1, feature2, correlation_value)\n\n    Notes:\n    ------\n    - Generates a correlation matrix heatmap\n    - Identifies feature pairs with absolute correlation > 0.85\n    - Only returns upper triangle of correlation matrix to avoid duplicates\n    - The heatmap uses a diverging color scheme centered at 0\n    \"\"\"\n\n    # Calculate correlation matrix\n    corr_matrix = df[numeric_features].corr()\n\n    # Plot correlation heatmap\n    plt.figure(figsize=(20, 20))\n    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, linewidth = 0.5)\n    plt.title('Feature Correlation Heatmap')\n    plt.xticks(rotation=90)\n    plt.yticks(rotation=0)\n    plt.show()\n\n    # Identify highly correlated features\n    threshold = threshold\n    high_corr = np.where(np.abs(corr_matrix) > threshold)\n    high_corr = [(corr_matrix.index[x], corr_matrix.columns[y], corr_matrix.iloc[x, y])\n                 for x, y in zip(*high_corr) if x != y and x < y]\n\n    return high_corr","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:01.111301Z","iopub.execute_input":"2025-10-17T04:44:01.111673Z","iopub.status.idle":"2025-10-17T04:44:01.131656Z","shell.execute_reply.started":"2025-10-17T04:44:01.111646Z","shell.execute_reply":"2025-10-17T04:44:01.130421Z"},"id":"wy5v4-4QKEt9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Variance analysis (df, numeric_features, target_col='Attack Type') – Levene -> dict with results\ndef analyze_variance_homogeneity(df, numeric_features, target_col='Attack Type'):  \n    \"\"\"  \n    Analyze the homogeneity of variances using Levene's test.  \n\n    Parameters:  \n    -----------  \n    df : pandas.DataFrame  \n        Input DataFrame containing the dataset.  \n    numeric_features : list  \n        List of column names containing numerical features to analyze. \n    target_col : str, optional (default='Attack Type')\n        Name of the target column\n\n    Returns:  \n    --------  \n    dict  \n        Dictionary containing the results of Levene's test for each feature.  \n    \"\"\"  \n    \n    results_levene = {}  \n    \n    for feature in numeric_features:  \n        # Group data by y and filter out groups with zero values  \n        groups = [group[feature].dropna().values for name, group in df.groupby(target_col)   \n                  if not group[feature].dropna().empty]  \n        \n        # Filter out groups that contain only zero values or have zero variance  \n        groups = [group for group in groups if len(group) > 0 and np.any(group != 0) and np.var(group) > 0]  \n        \n        # Check if there are at least two groups with valid data  \n        if len(groups) < 2:  \n            print(f\"Not enough valid groups to perform Levene's test for feature: {feature}\")  \n            continue  # Skip this feature if not enough valid groups  \n\n        # Perform Levene's Test  \n        stat_levene, p_value_levene = stats.levene(*groups)  \n        results_levene[feature] = {'Statistic': stat_levene, 'p-value': p_value_levene}  \n\n    return results_levene","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:01.133409Z","iopub.execute_input":"2025-10-17T04:44:01.133741Z","iopub.status.idle":"2025-10-17T04:44:01.150665Z","shell.execute_reply.started":"2025-10-17T04:44:01.133715Z","shell.execute_reply":"2025-10-17T04:44:01.149465Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Feature importance analysis (df, numeric_features, target_col='Attack Type') – Kruskal-Wallis -> df with H-statistics and p-values\ndef analyze_feature_importance(df, numeric_features, target_col='Attack Type'):\n    \"\"\"  \n    Analyze feature importance using the Kruskal-Wallis test.  \n\n    Parameters:  \n    -----------  \n    df : pandas.DataFrame  \n        Input DataFrame containing the dataset  \n    numeric_features : list  \n        List of column names containing numerical features to analyze  \n    target_col : str, optional (default='Attack Type')\n        Name of the target column\n\n    Returns:  \n    --------  \n    pandas.DataFrame  \n        DataFrame containing H-statistics and p-values for each feature,  \n        sorted by H-statistic in descending order  \n\n    Notes:  \n    ------  \n    - Uses the Kruskal-Wallis test to assess feature importance  \n    - Higher H-statistic indicates stronger relationship with target variable  \n    - Generates bar plot of H-statistics for visual comparison  \n    - Features are sorted by importance in the visualization  \n    \"\"\"  \n\n    h_scores = {}  \n    \n    for feature in numeric_features:  \n        # Group data by y and perform the Kruskal-Wallis test  \n        groups = [group[feature].dropna().values for name, group in df.groupby(target_col)]  \n        h_stat, p_val = stats.kruskal(*groups)  \n        h_scores[feature] = {'H-statistic': h_stat, 'p-value': p_val}  \n\n    # Create a DataFrame from the results  \n    h_scores_df = pd.DataFrame.from_dict(h_scores, orient='index')  \n    h_scores_df = h_scores_df.sort_values('H-statistic', ascending=False)  \n\n    # Plotting H-statistics  \n    plt.figure(figsize=(18, 10))  \n    plt.bar(range(len(h_scores_df)), h_scores_df['H-statistic'], color='skyblue')  \n    plt.xticks(range(len(h_scores_df)), h_scores_df.index, rotation=90)  \n    plt.title('Feature Importance based on H-statistic (Kruskal-Wallis Test)')  \n    plt.xlabel('Features')  \n    plt.ylabel('H-statistic')  \n    plt.tight_layout()  \n    plt.show()  \n\n    return h_scores_df","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:01.151415Z","iopub.execute_input":"2025-10-17T04:44:01.151779Z","iopub.status.idle":"2025-10-17T04:44:01.168897Z","shell.execute_reply.started":"2025-10-17T04:44:01.151752Z","shell.execute_reply":"2025-10-17T04:44:01.167707Z"},"id":"O-gl3dFkKH5B","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Feature importance analysis(df, numeric_features, target_col='Attack Type') – Tree Model -> df with features ordered by importance; confusion matrix; labels; cv_scores\ndef analyze_feature_importance_rf(df, numeric_features, target_col='Attack Type'):\n    \"\"\"\n    Analyze feature importance using a Random Forest classifier.\n\n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        Input DataFrame containing the dataset\n    numeric_features : list\n        List of column names containing numerical features to analyze\n    target_col : str\n        Column name of the target variable\n\n    Returns:\n    --------\n    pandas.DataFrame\n        DataFrame containing feature importances sorted by importance in descending order\n    cm\n        Confusion Matrix for further analysis\n    rf_labels\n        Labels for plotting the confusion matrix\n    cv_scores\n        Cross-validation scores for future reference\n\n    Notes:\n    ------\n    - Uses a Random Forest classifier to assess feature importance.\n    - Generates bar plot of feature importances for visual comparison.\n    \"\"\"\n\n    # Hyperparameter settings\n    hyperparameters = {\n        'n_estimators': 150,    # Number of trees\n        'max_depth': 30,        # Limit tree depth\n        'random_state': 42,     # For reproducibility\n        'n_jobs': -1            # Use all available cores\n    }\n\n    # Prepare the data\n    X = df[numeric_features]\n    y = df[target_col]\n\n    # Train/test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=hyperparameters['random_state'], stratify=y)\n\n    # Create the Random Forest model with specified hyperparameters\n    rf = RandomForestClassifier(**hyperparameters)\n\n    # Fit the model on the training set\n    rf.fit(X_train, y_train)\n\n    # Cross-validation on the training set\n    cv_scores = cross_val_score(rf, X_train, y_train, cv=5, n_jobs=-1)\n    print(f'Cross-Validation Score: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}')\n    \n    # Predict on test set\n    y_pred = rf.predict(X_test)\n\n    # Feature importances\n    importances = rf.feature_importances_\n    feature_importance_df = pd.DataFrame({'Feature': numeric_features, 'Importance': importances})\n    feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n\n    # Confusion matrix\n    rf_labels = rf.classes_\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Calculate test scores\n    report = classification_report(y_test, y_pred, target_names=rf_labels)\n    print(\"\\nClassification Report:\\n\")\n    print(report, end='\\n\\n')\n\n    # Plot feature importances\n    plt.figure(figsize=(18, 12))\n    plt.bar(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n    plt.ylabel('Importance')\n    plt.xlabel('Features')\n    plt.title('Feature Importance from Random Forest')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n\n    return feature_importance_df, cm, rf_labels, cv_scores","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:01.169989Z","iopub.execute_input":"2025-10-17T04:44:01.170366Z","iopub.status.idle":"2025-10-17T04:44:01.186389Z","shell.execute_reply.started":"2025-10-17T04:44:01.170295Z","shell.execute_reply":"2025-10-17T04:44:01.185257Z"},"id":"Ybfp8SRbKK9D","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Calculation of the percentage of outliers for each feature (df) – IQR -> dict with outliers percentage\ndef calculate_outliers_percentage(df):\n    \"\"\"\n    Calculate the percentage of outliers for each feature in the DataFrame using the IQR method.\n\n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        Input DataFrame containing the dataset.\n\n    Returns:\n    --------\n    dict\n        Dictionary containing the percentage of outliers for each feature.\n\n    Notes:\n    ------\n    - Uses the Interquartile Range (IQR) method to identify outliers.\n    - Outliers are defined as values below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR.\n    - The function returns a dictionary with feature names as keys and outlier percentages as values.\n    \"\"\"\n\n    outlier_percentages = {}  # empty dictionary to store outlier percentages\n\n    for column in df.columns:\n        Q1 = df[column].quantile(0.25)  # first quartile (Q1) for the column\n        Q3 = df[column].quantile(0.75)  # third quartile (Q3) for the column\n        IQR = Q3 - Q1  # Interquartile Range (IQR)\n\n        lower_bound = Q1 - 1.5 * IQR  # lower bound for outliers\n        upper_bound = Q3 + 1.5 * IQR  # upper bound for outliers\n\n        # Identify outliers in the column\n        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n\n        # Calculate the percentage of outliers for the column\n        outlier_percentage = len(outliers) / len(df) * 100\n\n        # Store the outlier percentage in the dictionary\n        outlier_percentages[column] = outlier_percentage\n\n    return outlier_percentages\n","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:01.187493Z","iopub.execute_input":"2025-10-17T04:44:01.187823Z","iopub.status.idle":"2025-10-17T04:44:01.207744Z","shell.execute_reply.started":"2025-10-17T04:44:01.187786Z","shell.execute_reply":"2025-10-17T04:44:01.206762Z"},"id":"v9A90OzwKSYH","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Initial Exploration\n\nThe first step is to load the dataset and examine its basic properties, such as dimensions, data types, and missing values.","metadata":{"id":"DlHUR2n9EvTc"}},{"cell_type":"markdown","source":"## 1.1. Loading the Dataset","metadata":{"id":"HKH56ronEvTc"}},{"cell_type":"code","source":"# List to store DataFrames\ndfs = []\n\n# Load the datasets\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        dfs.append(pd.read_csv(os.path.join(dirname, filename)))","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:01.210938Z","iopub.execute_input":"2025-10-17T04:44:01.211292Z","iopub.status.idle":"2025-10-17T04:44:37.239584Z","shell.execute_reply.started":"2025-10-17T04:44:01.211261Z","shell.execute_reply":"2025-10-17T04:44:37.238642Z"},"id":"wQofnfF6EvTc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data dimensions of each individual dataset\nfor i, data in enumerate(dfs, start=1):\n    rows, cols = data.shape\n    print(f'df{i} -> {rows} rows, {cols} columns')","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:37.241496Z","iopub.execute_input":"2025-10-17T04:44:37.241912Z","iopub.status.idle":"2025-10-17T04:44:37.249928Z","shell.execute_reply.started":"2025-10-17T04:44:37.241881Z","shell.execute_reply":"2025-10-17T04:44:37.248587Z"},"id":"LNVybGtREvTc","outputId":"d4a88d1e-0c1d-45fa-f03d-7d1d15b45c79","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.2. Merging the DataFrames\n\nAs noted by Panigrahi and Borah (2018), a typical IDS should be able to handle different types of attacks. Since the CICIDS2017 dataset is divided into eight files containing various attack data, these files should be merged into a single cohesive unit.","metadata":{"id":"Ha-P4K5LEvTd"}},{"cell_type":"code","source":"# Concatenate all DataFrames into a single DataFrame\ndata = pd.concat(dfs, axis=0, ignore_index=True)\n\n# Deleting DataFrames after merging\nfor df in dfs: del df","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:37.251024Z","iopub.execute_input":"2025-10-17T04:44:37.251281Z","iopub.status.idle":"2025-10-17T04:44:37.954854Z","shell.execute_reply.started":"2025-10-17T04:44:37.251258Z","shell.execute_reply":"2025-10-17T04:44:37.953893Z"},"id":"tgw6VhdQEvTd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.3. Data Overview","metadata":{"id":"kwBcdmM5EvTd"}},{"cell_type":"code","source":"# Display the first few rows\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:37.955849Z","iopub.execute_input":"2025-10-17T04:44:37.956137Z","iopub.status.idle":"2025-10-17T04:44:37.993295Z","shell.execute_reply.started":"2025-10-17T04:44:37.956112Z","shell.execute_reply":"2025-10-17T04:44:37.992200Z"},"id":"8YIUOAF3EvTd","outputId":"7c8a6344-5211-4c1a-b78e-d50d1e1293a6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display 10 random rows\ndata.sample(n=10, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:37.994392Z","iopub.execute_input":"2025-10-17T04:44:37.994768Z","iopub.status.idle":"2025-10-17T04:44:38.154046Z","shell.execute_reply.started":"2025-10-17T04:44:37.994741Z","shell.execute_reply":"2025-10-17T04:44:38.152942Z"},"id":"gOg5lFMJEvTd","outputId":"d69190b9-7981-4a9c-cff9-b9a799c8c2ce","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get dataset dimensions\nprint(f\"Dataset Dimensions: {data.shape}\")","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:38.155051Z","iopub.execute_input":"2025-10-17T04:44:38.155467Z","iopub.status.idle":"2025-10-17T04:44:38.161238Z","shell.execute_reply.started":"2025-10-17T04:44:38.155438Z","shell.execute_reply":"2025-10-17T04:44:38.159921Z"},"id":"QAXB4PFyEvTe","outputId":"3709175c-bfb9-4429-9aa6-2d0c446d362e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display data types\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:38.162399Z","iopub.execute_input":"2025-10-17T04:44:38.162787Z","iopub.status.idle":"2025-10-17T04:44:38.198333Z","shell.execute_reply.started":"2025-10-17T04:44:38.162745Z","shell.execute_reply":"2025-10-17T04:44:38.197313Z"},"id":"jrST0bI1EvTe","outputId":"f11c4544-1fe4-4a7b-dbc5-bde1ab301560","scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"By checking the data types, it is safe to assume the dataset countains only metric features, considering 'Label' as 'y' (the only categorical column).","metadata":{"id":"KvTyNeR2EvTe"}},{"cell_type":"code","source":"# Checking for missing values\nmissing_values = data.isna().sum()\nmissing_percentage = (missing_values / len(data)) * 100\n\n# Printing columns with missing values\nfor column, count in missing_values.items():\n    if count != 0:\n        print(f\"Column '{column}' has {count} missing values, which is {missing_percentage[column]:.2f}% of the total\")","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:38.199387Z","iopub.execute_input":"2025-10-17T04:44:38.199776Z","iopub.status.idle":"2025-10-17T04:44:38.617458Z","shell.execute_reply.started":"2025-10-17T04:44:38.199747Z","shell.execute_reply":"2025-10-17T04:44:38.616281Z"},"id":"mZLO47yUEvTe","outputId":"5056448e-de82-4d29-c3ab-ac8d2cbfcb56","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The number of missing values may initially seem irrelevant; however, we can decide on the approach after further analysis.","metadata":{"id":"ZWE3zyrdEvTe"}},{"cell_type":"markdown","source":"# 2. Data Cleaning","metadata":{"id":"TLBsIcXyEvTe"}},{"cell_type":"code","source":"# Removal of leading/trailing whitespace\ncol_names = {col: col.strip() for col in data.columns}\ndata.rename(columns = col_names, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:38.618547Z","iopub.execute_input":"2025-10-17T04:44:38.619021Z","iopub.status.idle":"2025-10-17T04:44:38.625018Z","shell.execute_reply.started":"2025-10-17T04:44:38.618988Z","shell.execute_reply":"2025-10-17T04:44:38.623780Z"},"id":"b-3Uj0NwEvTe","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.1. Duplicates and Infinite","metadata":{"id":"PWp_KxwAEvTe"}},{"cell_type":"code","source":"# Checking and counting duplicates\nduplicates = data.duplicated()\nduplicate_count = duplicates.sum()\n\n# Output results\nprint(f\"Number of duplicate rows: {duplicate_count}\")","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:38.626130Z","iopub.execute_input":"2025-10-17T04:44:38.626426Z","iopub.status.idle":"2025-10-17T04:44:55.934019Z","shell.execute_reply.started":"2025-10-17T04:44:38.626399Z","shell.execute_reply":"2025-10-17T04:44:55.932775Z"},"id":"w22CfZpHEvTe","outputId":"45105f47-65a2-4e4b-934a-14a4dafcba17","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Removing duplicates from a dataset is often considered safe and beneficial as it enhances data integrity, improves statistical accuracy, increases efficiency, and simplifies analysis.","metadata":{"id":"dQ9Z1lqLEvTe"}},{"cell_type":"code","source":"# Removal of duplicates\ndata = data.drop_duplicates(keep='first')\ndel duplicates\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:44:55.935143Z","iopub.execute_input":"2025-10-17T04:44:55.935528Z","iopub.status.idle":"2025-10-17T04:45:14.349961Z","shell.execute_reply.started":"2025-10-17T04:44:55.935490Z","shell.execute_reply":"2025-10-17T04:45:14.348898Z"},"id":"L17FTJHzEvTe","outputId":"e5457286-5cc4-4e52-8e26-8d58f4bacb04","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The same can be done for columns","metadata":{}},{"cell_type":"code","source":"# Identify columns with identical data\nidentical_columns = {}\ncolumns = data.columns\nlist_control = columns.copy().tolist()\n\n# Compare each pair of columns\nfor col1 in columns:\n    for col2 in columns:\n        if col1 != col2:\n            if data[col1].equals(data[col2]):\n                if (col1 not in identical_columns) and (col1 in list_control):\n                    identical_columns[col1] = [col2]\n                    list_control.remove(col2)\n                elif (col1 in identical_columns) and (col1 in list_control):\n                    identical_columns[col1].append(col2)\n                    list_control.remove(col2)\n\n# Print the result\nif identical_columns:\n    print(\"Identical columns found:\")\n    for key, value in identical_columns.items():\n        print(f\"'{key}' is identical to {value}\")\nelse: print(\"No identical columns found.\")","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:45:14.351123Z","iopub.execute_input":"2025-10-17T04:45:14.351563Z","iopub.status.idle":"2025-10-17T04:45:30.490604Z","shell.execute_reply.started":"2025-10-17T04:45:14.351536Z","shell.execute_reply":"2025-10-17T04:45:30.489642Z"},"id":"3ZCYOu-2EvTe","outputId":"57074bac-ae7a-447c-d2b7-a6f9456fe877","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Removing the columns with duplicated values\nfor key, value in identical_columns.items():\n    data.drop(columns=value, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:45:30.491518Z","iopub.execute_input":"2025-10-17T04:45:30.491807Z","iopub.status.idle":"2025-10-17T04:45:33.863074Z","shell.execute_reply.started":"2025-10-17T04:45:30.491783Z","shell.execute_reply":"2025-10-17T04:45:33.862076Z"},"id":"mfBVM7MsEvTe","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data.columns)\ndata.shape","metadata":{"id":"4Eq1owN-EvTe","outputId":"871a3362-eb12-435e-9252-59db4ce64a23","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:33.864092Z","iopub.execute_input":"2025-10-17T04:45:33.864360Z","iopub.status.idle":"2025-10-17T04:45:33.872428Z","shell.execute_reply.started":"2025-10-17T04:45:33.864337Z","shell.execute_reply":"2025-10-17T04:45:33.871185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking for infinite values\nnum_columns = data.select_dtypes(include = np.number).columns\nhas_infinite = np.isinf(data[num_columns]).sum()\nprint(has_infinite[has_infinite > 0])","metadata":{"id":"RNP0g_vSEvTe","outputId":"ffbee91b-abf4-4019-e90a-415ff4fe6d0b","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:33.873372Z","iopub.execute_input":"2025-10-17T04:45:33.873683Z","iopub.status.idle":"2025-10-17T04:45:36.246645Z","shell.execute_reply.started":"2025-10-17T04:45:33.873649Z","shell.execute_reply":"2025-10-17T04:45:36.245678Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Removing infinite values is typically safe and beneficial, as it enhances data integrity, ensures statistical accuracy, aids in proper model training, and clarifies insights.","metadata":{"id":"3-rBu5zZEvTf"}},{"cell_type":"code","source":"# Treating infinite values\ndata.replace([np.inf, -np.inf], np.nan, inplace=True)","metadata":{"id":"Crh2LOhqEvTf","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:36.251546Z","iopub.execute_input":"2025-10-17T04:45:36.251898Z","iopub.status.idle":"2025-10-17T04:45:37.724195Z","shell.execute_reply.started":"2025-10-17T04:45:36.251870Z","shell.execute_reply":"2025-10-17T04:45:37.723077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.1. Missing Values\n\nThere are different approaches to dealing with missing values. The first step in identifying how to proceed is to understand their impact on the dataset. Here, we will do that by analyzing the y column (Label).","metadata":{"id":"Pfaqes7QEvTf"}},{"cell_type":"code","source":"# Attack counts\nattack_counts = data['Label'].value_counts().reset_index()\nattack_counts.columns = ['Attack Type', 'Number of Occurrences']\n\n# Duplicating the df and dropping rows with missing values\ndata_no_na = data.dropna()\n\n# Counting the total number of occurrences of each attack after dropping\noccurrences_nonull = data_no_na['Label'].value_counts().reset_index()\noccurrences_nonull.columns = ['Attack Type', 'Occurrences w/o Null Rows']\n\n# Merging the DataFrames\nattack_counts = attack_counts.merge(occurrences_nonull, on='Attack Type', how='left')\n\n# Calculating the difference\nattack_counts['Abs Difference'] = attack_counts['Number of Occurrences'] - attack_counts['Occurrences w/o Null Rows']\nattack_counts['Difference %'] = ((attack_counts['Abs Difference'] * 100) / attack_counts['Number of Occurrences']).round(2)\n\n# Visualization\nattack_counts","metadata":{"id":"izRwr3pdEvTf","outputId":"b54d0bd9-8f14-46ea-a1e9-43854a7f1e9d","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:37.726569Z","iopub.execute_input":"2025-10-17T04:45:37.726994Z","iopub.status.idle":"2025-10-17T04:45:38.977487Z","shell.execute_reply.started":"2025-10-17T04:45:37.726960Z","shell.execute_reply":"2025-10-17T04:45:38.976406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cleaning up\ndel data_no_na\n\n# Evaluating percentage of missing values per column\nthreshold = 10\nmissing_percentage = (data.isnull().sum() / len(data)) * 100\n\n# Filter columns with missing values over the threshold\nhigh_missing_cols = missing_percentage[missing_percentage > threshold]\n\n# Print columns with high missing percentages\nif len(high_missing_cols) > 0:\n    print(f'The following columns have over {threshold}% of missing values:')\n    print(high_missing_cols)\nelse:\n    print('There are no columns with missing values greater than the threshold')","metadata":{"id":"HX6Gy6rFEvTf","outputId":"c267dd2f-9bb9-47df-805a-a6414c02b350","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:38.978635Z","iopub.execute_input":"2025-10-17T04:45:38.978968Z","iopub.status.idle":"2025-10-17T04:45:39.293079Z","shell.execute_reply.started":"2025-10-17T04:45:38.978939Z","shell.execute_reply":"2025-10-17T04:45:39.291737Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The analysis of missing values across the dataset suggests that missing values are not heavily concentrated in any single column and that the dataset can tolerate row-wise removal without significant impact.","metadata":{"id":"ckH8TNnbEvTf"}},{"cell_type":"code","source":"row_missing_percentage = (data.isna().sum(axis=1) / data.shape[1]) * 100\nprint(row_missing_percentage.describe())\n\nmissing_rows = data.isna().any(axis=1).sum()\nprint(f'\\nTotal rows with missing values: {missing_rows}')","metadata":{"id":"uYUCL2bFEvTf","outputId":"480bb564-92c8-4851-ff55-e73ce887766e","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:39.294333Z","iopub.execute_input":"2025-10-17T04:45:39.294788Z","iopub.status.idle":"2025-10-17T04:45:40.373283Z","shell.execute_reply.started":"2025-10-17T04:45:39.294748Z","shell.execute_reply":"2025-10-17T04:45:40.371930Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Following previous studies, such as Panigrahi and Borah (2018), and considering the minimal impact of removing rows with missing values (concentrated on the Benign class), we will proceed to drop these rows from the DataFrame.","metadata":{"id":"ItjIvQ36EvTf"}},{"cell_type":"code","source":"# Dropping missing values\ndata = data.dropna()\nprint(f'Dataset shape after row-wise removal: {data.shape}')","metadata":{"id":"lIAAhbphEvTf","outputId":"57c651cf-7779-4991-e5be-8f1bcb55bd9c","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:40.374817Z","iopub.execute_input":"2025-10-17T04:45:40.375209Z","iopub.status.idle":"2025-10-17T04:45:41.274058Z","shell.execute_reply.started":"2025-10-17T04:45:40.375172Z","shell.execute_reply":"2025-10-17T04:45:41.273132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2. Data-Driven Feature Selection\n\nThe idea here is to perform a straightforward manual feature selection process that focuses on examining the dataset critically.","metadata":{"id":"2Yj4g0m7EvTf"}},{"cell_type":"code","source":"# Check for numeric columns that contain only a single unique value, indicating no diversity in values.\n# Such columns contribute no useful information to the analysis and may be candidates for removal.\nonly_unique_cols = []\nfor col in data.columns:\n    if len(data[col].unique()) == 1:\n        only_unique_cols.append(col)\n        print(col)\n\nprint(f'\\nThe number of columns with only one unique values is: {len(only_unique_cols)}')","metadata":{"id":"6J-RhhcXEvTf","outputId":"70dc17d8-b222-441d-f475-48f6788206f1","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:41.274940Z","iopub.execute_input":"2025-10-17T04:45:41.275190Z","iopub.status.idle":"2025-10-17T04:45:43.944849Z","shell.execute_reply.started":"2025-10-17T04:45:41.275169Z","shell.execute_reply":"2025-10-17T04:45:43.943736Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Columns with only a single unique value can typically be removed from the dataset before training machine learning models without risk of loss of relevant information.","metadata":{"id":"Y4oEisRlEvTf"}},{"cell_type":"code","source":"# Dropping the columns with only one unique value\ndata.drop(only_unique_cols, axis=1, inplace=True)\ndel only_unique_cols","metadata":{"id":"094aDyNdEvTi","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:43.945833Z","iopub.execute_input":"2025-10-17T04:45:43.946122Z","iopub.status.idle":"2025-10-17T04:45:44.446376Z","shell.execute_reply.started":"2025-10-17T04:45:43.946098Z","shell.execute_reply":"2025-10-17T04:45:44.445243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking the new shape after cleaning\ndata.shape","metadata":{"id":"3Jn76UD8EvTi","outputId":"191fb7f8-6277-4fe1-ee07-3a1bc4bc0e1a","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:44.447469Z","iopub.execute_input":"2025-10-17T04:45:44.447849Z","iopub.status.idle":"2025-10-17T04:45:44.454236Z","shell.execute_reply.started":"2025-10-17T04:45:44.447815Z","shell.execute_reply":"2025-10-17T04:45:44.453028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.3. Grouping/ Removing Attack Types\n\nAs seen previously, the dataset categorizes various types of attacks into distinct groups, including different variations of Denial of Service (DoS) attacks. Given the imbalance present in the dataset, consolidating these classes could enhance the training process of ML models. This approach allows models to learn from a more balanced representation of the data, potentially improving classification accuracy and model performance.","metadata":{"id":"g0BdTKVOEvTi"}},{"cell_type":"markdown","source":"### 2.3.1. Grouping attack types\n\nThe table below illustrates the proposed grouping of attack categories. For now, DoS and DDoS will remain separate due to their distinct characteristics, such as differences in attack methodology and mitigation strategies, which could impact the model's performance and interpretation.\n\n\n| **Group**                     | **Attack Type**             | **Count**  |  \n|-------------------------------|-----------------------------|------------|  \n| **Normal Traffic**            | BENIGN                      | 2,095,057  |  \n|                               |                             | **Total: 2,095,057** |  \n| **DoS Attacks**              | DoS Hulk                   | 172,846    |  \n|                               | DoS GoldenEye               | 10,286     |  \n|                               | DoS Slowloris               | 5,385      |  \n|                               | DoS Slowhttptest            | 5,228      |\n|                               |                             | **Total: 193,745** |\n| **DDoS Attacks**              | DDoS                        | 128,014     |\n|                               |                             | **Total: 128,014** |\n| **Port Scanning**            | PortScan                    | 90,694     |  \n|                               |                             | **Total: 90,694**  |  \n| **Brute Force Attacks**      | FTP-Patator                 | 5,931      |  \n|                               | SSH-Patator                 | 3,219      |  \n|                               |                             | **Total: 9,150**   |  \n| **Bots**                      | Bot                         | 1,948      |  \n|                               |                             | **Total: 1,948**   |  \n| **Web Attacks**              | Web Attack – Brute Force    | 1,470      |  \n|                               | Web Attack – XSS            | 652        |  \n|                               | Web Attack – SQL Injection   | 21         |  \n|                               |                             | **Total: 2,143**   |  \n| **Infiltration Attacks**     | Infiltration                | 36         |  \n|                               |                             | **Total: 36**      |  \n| **Miscellaneous**            | Heartbleed                  | 11         |  \n|                               |                             | **Total: 11**      |","metadata":{"id":"Oua3PKpSEvTi"}},{"cell_type":"code","source":"# Mapping the attacks to the new group\ngroup_mapping = {\n    'BENIGN': 'Normal Traffic',\n    'DoS Hulk': 'DoS',\n    'DDoS': 'DDoS',\n    'PortScan': 'Port Scanning',\n    'DoS GoldenEye': 'DoS',\n    'FTP-Patator': 'Brute Force',\n    'DoS slowloris': 'DoS',\n    'DoS Slowhttptest': 'DoS',\n    'SSH-Patator': 'Brute Force',\n    'Bot': 'Bots',\n    'Web Attack � Brute Force': 'Web Attacks',\n    'Web Attack � XSS': 'Web Attacks',\n    'Infiltration': 'Infiltration',\n    'Web Attack � Sql Injection': 'Web Attacks',\n    'Heartbleed': 'Miscellaneous'\n}\n\n# Map to new group column\ndata['Attack Type'] = data['Label'].map(group_mapping)","metadata":{"id":"YSsOKbyCEvTj","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:44.455257Z","iopub.execute_input":"2025-10-17T04:45:44.455583Z","iopub.status.idle":"2025-10-17T04:45:44.658719Z","shell.execute_reply.started":"2025-10-17T04:45:44.455557Z","shell.execute_reply":"2025-10-17T04:45:44.657790Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking the new values\ndata['Attack Type'].value_counts()","metadata":{"id":"GgvTwtN_EvTj","outputId":"d32e36e1-71d6-4f20-a8f1-ad86b1a089ee","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:44.659698Z","iopub.execute_input":"2025-10-17T04:45:44.660047Z","iopub.status.idle":"2025-10-17T04:45:44.789673Z","shell.execute_reply.started":"2025-10-17T04:45:44.660014Z","shell.execute_reply":"2025-10-17T04:45:44.788373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dropping the old 'Label' column\ndata.drop(columns='Label', inplace=True)","metadata":{"id":"wStid6bUEvTj","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:44.790676Z","iopub.execute_input":"2025-10-17T04:45:44.790967Z","iopub.status.idle":"2025-10-17T04:45:45.300920Z","shell.execute_reply.started":"2025-10-17T04:45:44.790940Z","shell.execute_reply":"2025-10-17T04:45:45.299722Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.3.2. Removal of Attack Types\n\nGiven the very low instance counts for 'Infiltration' (36) and 'Heartbleed' (11), it is advisable to remove these categories from the dataset as they can lead to overfitting and unreliable model performance, even techniques like SMOTE may not be sufficient to create a representative training sample.","metadata":{"id":"LcF4LvoLEvTj"}},{"cell_type":"code","source":"# Removing rows with statistically irrelevant attack types\ndata.drop(data[(data['Attack Type'] == 'Infiltration') | (data['Attack Type'] == 'Miscellaneous')].index, inplace=True)","metadata":{"id":"ecu_pTGtEvTj","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:45.302407Z","iopub.execute_input":"2025-10-17T04:45:45.302813Z","iopub.status.idle":"2025-10-17T04:45:46.412591Z","shell.execute_reply.started":"2025-10-17T04:45:45.302782Z","shell.execute_reply":"2025-10-17T04:45:46.411419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data shape and attack counts after removal\nprint(data.shape)\ndata['Attack Type'].value_counts()","metadata":{"id":"0Gc_ExRGEvTj","outputId":"9e1766db-140b-4c2e-a77c-5221c4b5210c","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:46.413752Z","iopub.execute_input":"2025-10-17T04:45:46.414118Z","iopub.status.idle":"2025-10-17T04:45:46.545072Z","shell.execute_reply.started":"2025-10-17T04:45:46.414081Z","shell.execute_reply":"2025-10-17T04:45:46.544008Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Exploratory Data Analysis (EDA)","metadata":{"id":"Yag6m3Q-EvTj"}},{"cell_type":"markdown","source":"## 3.1. Descriptive Statistics","metadata":{"id":"72ZUuchSEvTj"}},{"cell_type":"code","source":"data.sample(10)","metadata":{"id":"WlAiJpN1EvTj","outputId":"cdcbfb5a-270a-49b2-ec64-6014abff230e","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:46.546079Z","iopub.execute_input":"2025-10-17T04:45:46.546345Z","iopub.status.idle":"2025-10-17T04:45:46.685994Z","shell.execute_reply.started":"2025-10-17T04:45:46.546323Z","shell.execute_reply":"2025-10-17T04:45:46.684898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.describe()","metadata":{"id":"WsgFPi_uEvTj","outputId":"09ec6fe0-07a1-40c5-992c-95a2143d7aa1","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:46.687016Z","iopub.execute_input":"2025-10-17T04:45:46.687405Z","iopub.status.idle":"2025-10-17T04:45:51.914789Z","shell.execute_reply.started":"2025-10-17T04:45:46.687364Z","shell.execute_reply":"2025-10-17T04:45:51.913361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2. Correlation Analysis\n\nThe correlation analysis provides valuable insights that could inform further data engineering efforts, such as considering the application of Principal Component Analysis (PCA) to reduce dimensionality and enhance model performance.","metadata":{"id":"PBeE_13bEvTj"}},{"cell_type":"code","source":"# Correlation Analysis:\nnumeric_features, categorical_features = get_feature_types(data)\nhigh_corr = correlation_analysis(data, numeric_features)","metadata":{"id":"mhgKoslgEvTj","outputId":"59fc91fd-b129-42eb-b96e-d3b89e5e7127","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:45:51.915838Z","iopub.execute_input":"2025-10-17T04:45:51.916168Z","iopub.status.idle":"2025-10-17T04:46:24.215210Z","shell.execute_reply.started":"2025-10-17T04:45:51.916128Z","shell.execute_reply":"2025-10-17T04:46:24.213967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Printing the pairs with high correlation and isolating the near/perfect multicollinearity\nhigh_multicollinearity = []\nfor item in high_corr:\n    print(f'{item[0]} has a high correlation with {item[1]}: {item[2].round(4)}')\n    if item[2] >= 0.95:\n        high_multicollinearity.append(item)","metadata":{"id":"dIPkE2hTFxHr","outputId":"dd995fa9-035b-4b2f-e60b-a3f4b2644887","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:46:24.216451Z","iopub.execute_input":"2025-10-17T04:46:24.216793Z","iopub.status.idle":"2025-10-17T04:46:24.235475Z","shell.execute_reply.started":"2025-10-17T04:46:24.216749Z","shell.execute_reply":"2025-10-17T04:46:24.228869Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting the pairs with very high correlation for better visualization\n\nn_plots = len(high_multicollinearity)\nn_cols = 4\nn_rows = (n_plots + n_cols - 1) // n_cols\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 4))\naxes = axes.flatten()\n\nfor i, item in enumerate(high_multicollinearity):\n    feature_x = item[0]\n    feature_y = item[1]\n    corr_value = item[2]\n\n    # Scatter plot\n    sns.scatterplot(x=data[feature_x], y=data[feature_y], ax=axes[i])\n    axes[i].set_title(f'{feature_x} vs {feature_y} (Corr={corr_value:.2f})', fontsize=8)\n    axes[i].set_xlabel(feature_x, fontsize=8)\n    axes[i].set_ylabel(feature_y, fontsize=8)\n\n# Hide any unused subplots\nfor j in range(len(high_multicollinearity), len(axes)):\n    fig.delaxes(axes[j])\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:46:24.236864Z","iopub.execute_input":"2025-10-17T04:46:24.237156Z","iopub.status.idle":"2025-10-17T04:48:49.185433Z","shell.execute_reply.started":"2025-10-17T04:46:24.237132Z","shell.execute_reply":"2025-10-17T04:48:49.183759Z"},"id":"hHfPbtWMFnme","outputId":"f2009b85-9093-4b03-b802-b5ca5c3a8e7d","trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"By evaluating the plots, we can identify some features with perfect/near-perfect linear correlation. Removing one instance can reduce redundancy and mitigate potential issues with multicollinearity in future models","metadata":{"id":"ayCY2Y1ltnXa"}},{"cell_type":"code","source":"# Removal of columns based on correlation analysis\nselected_columns = ['Total Backward Packets', 'Total Length of Bwd Packets', 'Subflow Bwd Bytes', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size']\n\n# dropping columns with perfect/near perfect multicollinearity\ndata.drop(columns=selected_columns, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:48:49.186836Z","iopub.execute_input":"2025-10-17T04:48:49.187108Z","iopub.status.idle":"2025-10-17T04:48:49.831819Z","shell.execute_reply.started":"2025-10-17T04:48:49.187085Z","shell.execute_reply":"2025-10-17T04:48:49.830858Z"},"id":"M42Rz4YhGEJP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Updating the variables and checking dataset shape\nnumeric_features, categorical_features = get_feature_types(data)\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:48:49.832784Z","iopub.execute_input":"2025-10-17T04:48:49.833042Z","iopub.status.idle":"2025-10-17T04:48:50.294827Z","shell.execute_reply.started":"2025-10-17T04:48:49.833020Z","shell.execute_reply":"2025-10-17T04:48:50.293651Z"},"id":"4rOCe3vDNM1O","outputId":"91cf1026-b95a-4e2b-f484-c1c6194365b0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3. Outliers\n\nOutlier detection plays a crucial role in ensuring the quality and reliability of a dataset. Identifying features with significant deviations from the norm can help improve model accuracy by preventing skewed results.","metadata":{"id":"w2UjimTqOagu"}},{"cell_type":"code","source":"# Calculate outliers percentage\noutlier_percentages = calculate_outliers_percentage(data[numeric_features])\n\n# Convert to DataFrame for easier manipulation\noutliers_df = pd.DataFrame.from_dict(outlier_percentages, orient='index', columns=['Outlier_Percentage'])\n\n# Define the threshold for concern\nthreshold = 10\n\n# Identify features with high percentage of outliers\nhigh_outlier_features = outliers_df[outliers_df['Outlier_Percentage'] > threshold]\n\n# Plot the outlier percentages and highlight features above the threshold\nplt.figure(figsize=(15, 10))\noutliers_df.sort_values(by='Outlier_Percentage', ascending=False).plot(kind='bar', legend=False, figsize=(20, 5))\nplt.axhline(y=threshold, color='r', linestyle='--', label=f'{threshold}% Threshold')\nplt.xlabel('Features')\nplt.ylabel('Percentage of Outliers')\nplt.title('Percentage of Outliers for Each Feature with Threshold')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:48:50.295936Z","iopub.execute_input":"2025-10-17T04:48:50.296265Z","iopub.status.idle":"2025-10-17T04:49:02.265807Z","shell.execute_reply.started":"2025-10-17T04:48:50.296230Z","shell.execute_reply":"2025-10-17T04:49:02.264600Z"},"id":"ixnfL1HyOZvz","outputId":"b22c4950-4e3b-461a-db17-f21cf924595f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the features with high outlier percentages\nprint(f\"Features with outlier percentage above {threshold}%:\\n\")\nprint(high_outlier_features.sort_values('Outlier_Percentage', ascending=False))\n\n# Cleaning up\ndel outliers_df","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:49:02.267047Z","iopub.execute_input":"2025-10-17T04:49:02.267337Z","iopub.status.idle":"2025-10-17T04:49:02.276036Z","shell.execute_reply.started":"2025-10-17T04:49:02.267309Z","shell.execute_reply":"2025-10-17T04:49:02.274717Z"},"id":"uwp-lW-BO7-H","outputId":"28c7f1ff-5d71-4de6-dfab-36dd4acf5db9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.4. Data Distribution\n\nUnderstanding the distribution of the data is fundamental for selecting the right preprocessing and modeling techniques. Data distribution also gives insights towards the best statistic tests to perform.","metadata":{"id":"049NKw1jPQt8"}},{"cell_type":"code","source":"norm_dist = 0\nnot_norm_dist = 0\n\nfor col in numeric_features:\n\n    # Perform Anderson-Darling test for normality\n    result = stats.anderson(data[col], dist='norm')\n\n    # Compare the statistic with the critical value at 5% significance level\n    if result.statistic < result.critical_values[2]:  # 5% significance level\n        norm_dist += 1\n    else:\n        not_norm_dist += 1\n\nprint(f'{norm_dist} features are normally distributed')\nprint(f'{not_norm_dist} features are not normally distributed - Reject null hypothesis')","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:49:02.277300Z","iopub.execute_input":"2025-10-17T04:49:02.277732Z","iopub.status.idle":"2025-10-17T04:49:36.632424Z","shell.execute_reply.started":"2025-10-17T04:49:02.277692Z","shell.execute_reply":"2025-10-17T04:49:36.631422Z"},"id":"kL0RL_02PVOl","outputId":"ec9928fa-6b64-4a25-c240-5a950abcf093","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.5. Class Imbalance","metadata":{"id":"5zoyONkMEvTk"}},{"cell_type":"code","source":"# Count occurrences of each attack type and convert to a DataFrame\nattack_counts_df = data['Attack Type'].value_counts().reset_index()\nattack_counts_df.columns = ['Attack Type', 'Number of Occurrences']\n\n# Counting the total for each attack on both cases\ntotal_occurrences = attack_counts_df['Number of Occurrences'].sum()\n\n# Calculating the respective percentages\nattack_counts_df['% of Total'] = ((attack_counts_df['Number of Occurrences'] / total_occurrences) * 100).round(2)\n\nprint(attack_counts_df)","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:49:36.633818Z","iopub.execute_input":"2025-10-17T04:49:36.634204Z","iopub.status.idle":"2025-10-17T04:49:36.766765Z","shell.execute_reply.started":"2025-10-17T04:49:36.634168Z","shell.execute_reply":"2025-10-17T04:49:36.765512Z"},"id":"LGtjj3-FEvTk","outputId":"9afc4aca-5b9e-458b-a975-b9141890ed31","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Class Imbalance**\n\nThe previous analysis show a clear class imbalance, which should be taken into consideration while working with the CIC-IDS-2017, as highlighted by Panigrahi and Borah (2018).\n\n**Observations**:\n1. Dominance of the 'BENIGN' class/ Normal Traffic – 83.1% of the entire dataset\n2. Higher types with lower counts – Even DoS, the second most relevant class, shows low counts\n3. Rare classes even after regrouping, such as Bots, Web Attacks and Brute Force\n\n**Potential implications**:\n* Model Bias: Models can become biased towards the majority class, leading to high accuracy but poor recall and precision for minority classes.\n* Performance Metrics: In an imbalanced dataset, accuracy is often not a sufficient metric to evaluate model performance. F1-score, precision, recall, or ROC-AUC are better suited for assessing the model's ability to classify different classes effectively.\n\n**Ideas to address the issue during ML training**:\n* Class weights to penalize mistakes on minority classes\n* Ensemble Methods can improve overall predictions (Random Forests, XGBoost)\n* Cross-validation with representative folds\n* Partial SMOTE to help balancing the distribution\n* Undersampling the majority class","metadata":{"id":"wekKwHJ0EvTk"}},{"cell_type":"code","source":"# Cleaning up\ndel attack_counts_df","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:49:36.767555Z","iopub.execute_input":"2025-10-17T04:49:36.767920Z","iopub.status.idle":"2025-10-17T04:49:36.786585Z","shell.execute_reply.started":"2025-10-17T04:49:36.767887Z","shell.execute_reply":"2025-10-17T04:49:36.785452Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Feature Importance","metadata":{"id":"suph7itLEvTl"}},{"cell_type":"markdown","source":"Before conducting an analysis of feature importance, some characteristics of the dataset must be gathered. We already know it does not follow a Gaussian distribution and has a high presence of outliers. The next step is to evaluate its variance to determine the best statistical test.","metadata":{}},{"cell_type":"code","source":"# Applying the Levene's Test\n# p-value < 0.05 suggests unequal variances among groups (rejecting the null hypothesis of equal variances)\nvariance_result = analyze_variance_homogeneity(data, numeric_features)","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:49:36.787643Z","iopub.execute_input":"2025-10-17T04:49:36.787959Z","iopub.status.idle":"2025-10-17T04:50:24.773455Z","shell.execute_reply.started":"2025-10-17T04:49:36.787933Z","shell.execute_reply":"2025-10-17T04:50:24.772458Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analysing the results\nno_significant_results = True\n\nfor feature, result in variance_result.items():   \n    if result['p-value'] > 0.05:  \n        print(f\"\\nFeature: {feature}\")  \n        print(f\"  - Test Statistic: {result['Statistic']:.4f}\")  \n        print(f\"  - p-value: {result['p-value']:.4f}\")  \n        print(\"  - Interpretation: The variances are not significantly different (fail to reject null hypothesis).\")  \n        no_significant_results = False\n\n# If no features had p-values > 0.05  \nif no_significant_results:  \n    print(\"\\nNo features have p-values greater than 0.05. All features have significant differences in variance.\")","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:50:24.774110Z","iopub.execute_input":"2025-10-17T04:50:24.774349Z","iopub.status.idle":"2025-10-17T04:50:24.784070Z","shell.execute_reply.started":"2025-10-17T04:50:24.774328Z","shell.execute_reply":"2025-10-17T04:50:24.782973Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The above evaluation (only 3 features failed to reject the null hypothesis), together with the information about distribution, invalidates the use of One-Way ANOVA. A better fit, therefore, is the Kruskal-Wallis test, which is a non-parametric alternative that does not assume equal variances and is robust to the violations of normality or homoscedasticity.","metadata":{}},{"cell_type":"code","source":"# Analysing feature importance using Kruskal-Wallis.\nh_p_stats = analyze_feature_importance(data, numeric_features)","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:50:24.785182Z","iopub.execute_input":"2025-10-17T04:50:24.785528Z","iopub.status.idle":"2025-10-17T04:51:33.101471Z","shell.execute_reply.started":"2025-10-17T04:50:24.785501Z","shell.execute_reply":"2025-10-17T04:51:33.100386Z"},"id":"ujmASzpzEvTl","outputId":"5ddba3d7-2bdb-497f-e90c-1cf351debccf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that we have the results from the Kruskal-Wallis test, we will train a Random Forest model and analyze its feature importance scores. This will provide an additional perspective on feature relevance, which we can then compare to the rankings obtained from the Kruskal-Wallis test.","metadata":{"id":"pVcrrYHJEvTl"}},{"cell_type":"code","source":"# Analysing feature importance using Tree-Model\nfeature_importance_tree, cm, rf_labels, cv_scores = analyze_feature_importance_rf(data, numeric_features)","metadata":{"execution":{"iopub.status.busy":"2025-10-17T04:51:33.102815Z","iopub.execute_input":"2025-10-17T04:51:33.103207Z"},"id":"xiQgKawUEvTl","outputId":"5fc19f23-c575-4300-97ce-b9ed5d0422bc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Random Forests, being ensemble methods, offer a built-in mechanism for assessing feature importance. This importance is calculated based on how much each feature contributes to decreasing impurity (e.g., Gini impurity or entropy) within the decision trees of the forest. Features used higher up in the trees or in more trees of the forest for making splits are considered more important as they contribute more significantly to the model's predictive performance.","metadata":{"id":"Z42L-e6NEvTl"}},{"cell_type":"code","source":"# Evaluating RF performance based on selected features\n# Plot the confusion matrix as a heatmap\nplt.figure(figsize=(10, 8)) # Adjust figure size as needed\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=rf_labels, yticklabels=rf_labels)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After evaluating the Random Forest model's performance, we observe an overall positive result, indicating its suitability for feature importance analysis. However, the confusion matrix and the classification report reveal a notable weakness in classifying Bot attacks, with roughly 1/4 of these instances being misclassified as normal traffic (which is not surprising, considering this is a minority class). This misclassification highlights a potential area for improvement in our final intrusion detection model, which could be explored by considering more sophisticated bot detection methods, specialized feature engineering, and data balancing techniques. Despite this limitation, the overall performance justifies using the Random Forest's feature importance scores as a valuable reference for comparison with the Kruskal-Wallis H-statistic in our subsequent feature selection process.","metadata":{}},{"cell_type":"code","source":"# Preparing the df for comparison\nh_p_stats.reset_index(inplace = True)\nh_p_stats.rename(columns = {'index':'Feature'}, inplace = True)\n\n# Sorting the new df for easier visualization\ncomparison_tb = feature_importance_tree.merge(h_p_stats, on = 'Feature', how = 'left')\ncomparison_tb_sorted = comparison_tb.sort_values(by='Importance', ascending=False)\ncomparison_tb_sorted","metadata":{"id":"44txOIcOEvTl","trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create figure and axis\nplt.figure(figsize=(25, 10))\n\n# Create a color map based on p-value\ncolors = sns.color_palette(\"coolwarm\", as_cmap=True)\n\n# Create twin axes for different scales\nax1 = plt.gca()\nax2 = ax1.twinx()\n\n# Plot bars for importance\nbars = ax1.bar(comparison_tb_sorted['Feature'],\n                comparison_tb_sorted['Importance'],\n                alpha=0.6,\n                color=[colors(0 + (value)) for value in comparison_tb_sorted['p-value']], edgecolor='black')\n\n# Plot line for H-statistic\nline = ax2.plot(comparison_tb_sorted['Feature'],\n                 comparison_tb_sorted['H-statistic'],\n                 color='black',\n                 linewidth=2,\n                 label='H-statistic',\n                 marker='o')\n\n# Add a horizontal threshold line\nthreshold = 100000\nax2.axhline(y=threshold, color='red', linestyle='--', label=f'H-statistic Threshold ({threshold})')\nax2.legend()\n\n# Customize primary y-axis (Importance)\nax1.set_ylabel('Feature Importance', fontsize=12, color='black')\nax1.tick_params(axis='y', labelcolor='black')\n\n# Set x-ticks and labels\nax1.set_xticks(range(len(comparison_tb_sorted['Feature'])))\nax1.set_xticklabels(comparison_tb_sorted['Feature'], rotation=90, ha='center', fontsize=10)\n\n# Add title\nplt.title('Feature Importance from Random Forest with H-statistics and p-values',\n          fontsize=14, pad=20)\n\n# Add colorbar for p-values\nsm = plt.cm.ScalarMappable(cmap=colors, norm=plt.Normalize(vmin=comparison_tb_sorted['p-value'].min(), vmax=0.1))\ncbar = plt.colorbar(sm, ax=ax1, orientation='vertical')\ncbar.set_label('p-value', fontsize=10)\n\n# Add gridlines for readability\nax1.grid(True, which='both', axis='y', linestyle='--', linewidth=0.7)\nax2.grid(True, which='both', axis='y', linestyle='--', linewidth=0.7)\n\nplt.show()\n","metadata":{"id":"CyQ8YH5bEvTl","outputId":"89694269-94d0-4891-e848-60e88452039f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The plot illustrates that while some features selected by the Random Forest (RF) have lower H-statistics, the most significant features are found within the range where the highest H-statistic values are concentrated. The most interesting information, however, lies on the right tail, where there is a convergence on the lowest priority and near-zero H-statistic relevance. Based on these results, the least important features will be removed from the dataset, which helps with:\n\n * Improving Model Efficiency: Training with fewer features reduces computational cost (time and memory).\n * Reducing Noise: Removing irrelevant features can sometimes improve model performance by reducing noise and preventing overfitting.\n * Simplifying Interpretation: Models with fewer features are often easier to interpret and explain.","metadata":{}},{"cell_type":"code","source":"# Removing statiscally irrelavant features from the dataset\ncols_to_remove = ['ECE Flag Count', 'RST Flag Count', 'Fwd URG Flags', 'Idle Std', 'Fwd PSH Flags', 'Active Std', 'Down/Up Ratio', 'URG Flag Count']\ndata.drop(columns=cols_to_remove, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.1. Overview of the Final Columns","metadata":{"scrolled":true}},{"cell_type":"markdown","source":"| **Column Name**                    | **Description**                                                                                                                                             |\n|-------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Destination Port                    | The destination port for the flow.                                                                                                                         |\n| Flow Duration                       | The duration of the flow.                                                                                                                                   |\n| Total Fwd Packets                   | The total number of forwarded packets in the flow.                                                                                                         |\n| Total Length of Fwd Packets         | The total length of forwarded packets in the flow.                                                                                                         |\n| Fwd Packet Length Max               | The maximum size of forwarded packets.                                                                                                                      |\n| Fwd Packet Length Min               | The minimum size of forwarded packets.                                                                                                                     |\n| Fwd Packet Length Mean              | The average length of forwarded packets in the flow.                                                                                                       |\n| Fwd Packet Length Std               | The standard deviation of forwarded packet lengths in the flow.                                                                                           |\n| Bwd Packet Length Max               | The maximum size of backward packets.                                                                                                                       |\n| Bwd Packet Length Min               | The minimum size of backward packets.                                                                                                                     |\n| Bwd Packet Length Mean              | The average size of backward packets.                                                                                                                       |\n| Bwd Packet Length Std               | The standard deviation of backward packet lengths in the flow.                                                                                             |\n| Flow Bytes/s                        | The number of bytes transferred per second in the flow.                                                                                                   |\n| Flow Packets/s                      | The number of packets transferred per second in the flow.                                                                                                  |\n| Flow IAT Mean                       | The average inter-arrival time of packets in the flow.                                                                                                     |\n| Flow IAT Std                        | The standard deviation of the inter-arrival times of packets in the flow.                                                                                  |\n| Flow IAT Max                        | The maximum inter-arrival time of packets in the flow.                                                                                                      |\n| Flow IAT Min                        | The minimum inter-arrival time of packets in the flow.                                                                                                     |\n| Fwd IAT Total                       | The total forward inter-arrival time observed during the flow.                                                                                             |\n| Fwd IAT Mean                        | The mean forward inter-arrival time observed during the flow.                                                                                              |\n| Fwd IAT Std                         | The standard deviation of the forward inter-arrival time during the flow.                                                                                 |\n| Fwd IAT Max                         | The maximum forward inter-arrival time observed during the flow.                                                                                           |\n| Fwd IAT Min                         | The minimum forward inter-arrival time observed during the flow.                                                                                           |\n| Bwd IAT Total                       | The total backward inter-arrival time during the flow.                                                                                                    |\n| Bwd IAT Mean                        | The average backward inter-arrival time during the flow.                                                                                                  |\n| Bwd IAT Std                         | The standard deviation of the backward inter-arrival time during the flow.                                                                                 |\n| Bwd IAT Max                         | The maximum backward inter-arrival time during the flow.                                                                                                  |\n| Bwd IAT Min                         | The minimum backward inter-arrival time during the flow.                                                                                                  |\n| Fwd Header Length                   | The total length of the header in forwarded packets.                                                                                                       |\n| Bwd Header Length                   | The total length of the header in backward packets.                                                                                                        |\n| Fwd Packets/s                       | The number of forwarded packets transferred per second in the flow.                                                                                        |\n| Bwd Packets/s                       | The number of backward packets transferred per second in the flow.                                                                                        |\n| Min Packet Length                   | The minimum packet length observed in the flow.                                                                                                            |\n| Max Packet Length                   | The maximum packet length observed in the flow.                                                                                                            |\n| Packet Length Mean                  | The average packet length in the flow.                                                                                                                     |\n| Packet Length Std                   | The standard deviation of packet lengths in the flow.                                                                                                      |\n| Packet Length Variance              | The variance of packet lengths in the flow.                                                                                                                |\n| FIN Flag Count                      | The number of FIN flags observed in the flow.                                                                                                               |\n| PSH Flag Count                      | The number of PSH flags observed in the flow.                                                                                                               |\n| ACK Flag Count                      | The number of ACK flags observed in the flow.                                                                                                               |\n| Average Packet Size                 | The average size of the packets in the flow.                                                                                                               |\n| Subflow Fwd Bytes                   | The number of bytes in the forwarded subflow.                                                                                                              |\n| Init_Win_bytes_forward              | The initial window size in bytes in the forward direction.                                                                                                 |\n| Init_Win_bytes_backward             | The initial window size in bytes in the backward direction.                                                                                                |\n| act_data_pkt_fwd                    | The number of active data packets forwarded.                                                                                                               |\n| min_seg_size_forward                | The minimum segment size forwarded in the flow.                                                                                                            |\n| Active Mean                         | The average amount of activity observed during the flow.                                                                                                   |\n| Active Max                          | The maximum amount of activity observed during the flow.                                                                                                   |\n| Active Min                          | The minimum amount of activity observed during the flow.                                                                                                   |\n| Idle Mean                           | The average idle time observed during the flow.                                                                                                            |\n| Idle Max                            | The maximum idle time observed during the flow.                                                                                                            |\n| Idle Min                            | The minimum idle time observed during the flow.                                                                                                            |\n| Attack Type                         | The target class, indicating the type of attack or normal traffic.                                                                                        |\n","metadata":{}},{"cell_type":"markdown","source":"# 6. Outputs","metadata":{"id":"MJQdKSutEvTl"}},{"cell_type":"markdown","source":"**Note on Sampling and Scaling Techniques:**\nApplying resampling and/or scaling techniques, such as SMOTE and RobustScaler, to the entire dataset would violate the principle of keeping the test set unseen during model training. This could lead to overly optimistic performance metrics, as the model might indirectly gain information about the test set during training. Based on the analysis performed on this notebook, both techniques can benefit when fitting models. However, these methods will only be implemented on the [next notebook](https://www.kaggle.com/code/ericanacletoribeiro/cicids2017-machine-learning-models-comparison), when the dataset will be splitted into training and test sets, focusing on training and comparing machine learning models.","metadata":{}},{"cell_type":"code","source":"# Clean dataset to a CSV file\ndata.to_csv('cicids2017_cleaned.csv', index=False)","metadata":{"id":"zyp0O-2VEvTm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This dataset has been made available on Kaggle [here](https://www.kaggle.com/datasets/ericanacletoribeiro/cicids2017-cleaned-and-preprocessed).\n\nThe complete pipeline is [available on Github](https://github.com/anacletu/ml-intrusion-detection-cicids2017).","metadata":{}},{"cell_type":"markdown","source":"# References\n\nIman Sharafaldin, Arash Habibi Lashkari, and Ali A. Ghorbani, “Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization”, 4th International Conference on Information Systems Security and Privacy (ICISSP), Portugal, January 2018.\n\nPanigrahi, R.; Borah, S. 2018. A detailed analysis of CICIDS2017 dataset for designing intrusion detection systems. International Journal of Engineering & Technology 7(3.24): 479-482.","metadata":{"id":"27kTwPSDEvTm"}}]}